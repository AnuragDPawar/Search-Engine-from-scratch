{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import time\n",
    "input_path='files'\n",
    "output_dir_path_gloabl='P1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to write the global output file (combination of all 503 files)\n",
    "def generate_output_file_global(filename,output_dir_path_gloabl,token_dict):\n",
    "    filename = 'out' + '-' + os.path.splitext(filename)[0]+'.txt'\n",
    "    filepath = os.path.join(output_dir_path_gloabl, filename)\n",
    "    with open(filename, \"w\") as file:\n",
    "        for key, value in token_dict.items():\n",
    "            file.write(\"{} {}\\n\".format(key,value))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to write individual file\n",
    "def generate_output_file(filename,output_dir_path,token_dict):\n",
    "    filename = 'out' + '-' + os.path.splitext(filename)[0]+'.txt'\n",
    "    filepath = os.path.join(output_dir_path, filename)\n",
    "    with open(filepath, \"w\") as file:\n",
    "        for key, value in token_dict.items():\n",
    "            file.write(\"{} {}\\n\".format(key,value))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kill all script and style elements\n",
    "def html_cleaner(path):\n",
    "    #reference: https://docs.python.org/2.4/lib/standard-encodings.html \n",
    "    test_file = open(path,'r',encoding='unicode_escape') \n",
    "    #parsing the html file\n",
    "    soup = BeautifulSoup(test_file, features=\"html.parser\")\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.decompose()    # rip it out\n",
    "    text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "  #cleaning the parsed file\n",
    "  return re.sub(r\"[^a-zA-Z]\",\"\",string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file(input_path,time_intervals,final_time):\n",
    "    i=0\n",
    "    curr_time=0.0 #to hold the time\n",
    "    global_dict = defaultdict()\n",
    "    for filename in os.listdir(input_path):\n",
    "        filepath = os.path.join(input_path, filename)\n",
    "        temp= html_cleaner(filepath) #calling parsing function\n",
    "        start_time = time.time()\n",
    "        tokens=nltk.tokenize.word_tokenize(temp,language='english', preserve_line=False)\n",
    "        my_dict = defaultdict()\n",
    "        for curr_word in tokens:\n",
    "            curr_word = clean(curr_word) #calling the cleaning function\n",
    "            # started recording the processing time            \n",
    "            if curr_word:\n",
    "                curr_word = curr_word.lower()\n",
    "                my_dict[curr_word] = 1 + my_dict.get(curr_word,0) #calculating the frequncy of the tokens\n",
    "                global_dict[curr_word] = 1 + my_dict.get(curr_word,0)\n",
    "        #stopped recording the processing time\n",
    "        stop_time = time.time() - start_time\n",
    "\n",
    "        sorted_alphabetically = dict(sorted(my_dict.items(),key=lambda x:x[0])) #sorting alphabetically\n",
    "        generate_output_file(filename,'output',sorted_alphabetically)\n",
    "        curr_time += stop_time\n",
    "        if time_intervals and i+1 == time_intervals[0]:\n",
    "            final_time += curr_time\n",
    "            curr_time=0\n",
    "            print(\"Time taken for {} files: {}\".format(i+1,final_time)) #calculating the time\n",
    "            time_intervals.pop(0)\n",
    "        i+=1\n",
    "    global_sorted_alphabetically = dict(sorted(global_dict.items(),key=lambda x:x[1]))\n",
    "    generate_output_file_global('gloabal_op_1','output',global_sorted_alphabetically)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution():\n",
    "    #path of the input file folder\n",
    "    input_path='files' \n",
    "    #to record the processing time\n",
    "    time_intervals = [50,100,150,200,250,300,350,400,450,500]\n",
    "    final_time=0.0\n",
    "    generate_file(input_path,time_intervals,final_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 50 files: 1.2161293029785156\n",
      "Time taken for 100 files: 1.8004472255706787\n",
      "Time taken for 150 files: 2.526153564453125\n",
      "Time taken for 200 files: 3.184561014175415\n",
      "Time taken for 250 files: 4.127193212509155\n",
      "Time taken for 300 files: 5.310114860534668\n",
      "Time taken for 350 files: 7.58649206161499\n",
      "Time taken for 400 files: 10.5447256565094\n",
      "Time taken for 450 files: 13.202758312225342\n",
      "Time taken for 500 files: 13.641358852386475\n"
     ]
    }
   ],
   "source": [
    "solution()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c1323188123de946c5b7b5e25c9bb76ffbaea461ceaf195e45c5d976a7611c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
